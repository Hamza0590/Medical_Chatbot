# Medical Chatbot Project - Complete Learning Guide

**Project**: Advanced Medical Chatbot with Multi-Query RAG  
**Author**: Hamza  
**Date**: November 2025

---

## Table of Contents

1. [Core Concepts](#core-concepts)
2. [LangChain Framework](#langchain-framework)
3. [Vector Databases & Embeddings](#vector-databases--embeddings)
4. [Advanced RAG Techniques](#advanced-rag-techniques)
5. [LLM Integration](#llm-integration)
6. [Web Development](#web-development)
7. [System Architecture](#system-architecture)
8. [Algorithms & Data Structures](#algorithms--data-structures)

---

## Core Concepts

### 1. RAG (Retrieval Augmented Generation)

**What it is**: A technique that combines information retrieval with text generation.

**How it works**:
1. Retrieve relevant documents from a knowledge base
2. Augment the LLM prompt with retrieved context
3. Generate answer based on both context and query

**Why use it**:
- âœ… Reduces hallucinations
- âœ… Grounds responses in factual data
- âœ… Allows updating knowledge without retraining
- âœ… More cost-effective than fine-tuning

**In your project**:
```python
# Retrieve documents
docs = retriever.invoke(query)
context = "\n\n".join(d.page_content for d in docs)

# Augment prompt with context
prompt = f"Context: {context}\n\nQuestion: {query}"

# Generate response
response = llm.invoke(prompt)
```

---

### 2. Multi-Query RAG

**What it is**: Generating multiple query variations to improve retrieval quality.

**Process**:
1. Original query: "What is diabetes?"
2. Generate variations:
   - "Explain diabetes mellitus"
   - "Define diabetic condition"
   - "What causes diabetes?"
3. Retrieve documents for each variation
4. Combine and re-rank results

**Benefits**:
- Better recall (finds more relevant docs)
- Handles different phrasings
- More robust to query formulation

**In your project**:
```python
def generate_query_variations(original_query: str, num_variations: int = 5):
    llm = ChatGroq(model_name="llama-3.3-70b-versatile", temperature=0.7)
    
    prompt = ChatPromptTemplate.from_messages([
        SystemMessagePromptTemplate.from_template(
            "Generate {num_variations} alternative ways to ask: {query}"
        )
    ])
    
    response = llm.invoke(prompt.format_messages(...))
    return [original_query] + variations
```

---

### 3. Reciprocal Rank Fusion (RRF)

**What it is**: An algorithm for combining multiple ranked lists.

**Formula**:
```
RRF_score(doc) = Î£ [1 / (k + rank)]
where k = 60 (constant)
```

**Example**:
```
Query 1: [Doc A(rank 1), Doc B(rank 2), Doc C(rank 3)]
Query 2: [Doc A(rank 1), Doc D(rank 2), Doc B(rank 3)]

Doc A score: 1/(60+1) + 1/(60+1) = 0.0328 â† Highest!
Doc B score: 1/(60+2) + 1/(60+3) = 0.0320
```

**Why use it**:
- No training required
- Simple and fast
- Proven effective in search engines
- Handles multiple result lists naturally

**In your project**:
```python
def reciprocal_rank_fusion(doc_lists: List[List[any]], k: int = 60):
    doc_scores = defaultdict(float)
    
    for doc_list in doc_lists:
        for rank, doc in enumerate(doc_list, start=1):
            doc_id = doc.page_content
            doc_scores[doc_id] += 1.0 / (k + rank)
    
    ranked_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)
    return ranked_docs
```

---

### 4. Vector Embeddings

**What it is**: Converting text into numerical vectors that capture semantic meaning.

**Key properties**:
- Similar texts have similar vectors
- Enables semantic search (meaning-based, not keyword-based)
- Fixed-size representation (e.g., 384 dimensions)

**Example**:
```
"diabetes" â†’ [0.23, -0.45, 0.67, ..., 0.12]  (384 numbers)
"diabetic" â†’ [0.25, -0.43, 0.69, ..., 0.11]  (very similar!)
"apple"    â†’ [-0.89, 0.34, -0.12, ..., 0.56] (very different)
```

**In your project**:
```python
from langchain_huggingface import HuggingFaceEmbeddings

embeddings_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

# Convert text to vector
vector = embeddings_model.embed_query("What is diabetes?")
# Returns: array of 384 numbers
```

---

### 5. Semantic Search

**What it is**: Searching by meaning rather than exact keywords.

**Traditional keyword search**:
```
Query: "heart attack"
Finds: documents with exact words "heart" and "attack"
Misses: "myocardial infarction", "cardiac arrest"
```

**Semantic search**:
```
Query: "heart attack"
Finds: "myocardial infarction", "cardiac arrest", "coronary event"
Because they have similar meaning!
```

**How it works**:
1. Convert query to vector
2. Find documents with similar vectors (cosine similarity)
3. Return top-k most similar

**In your project**:
```python
# Pinecone automatically does semantic search
retriever = docsearch.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 5}
)

docs = retriever.invoke("heart attack")
# Returns documents about myocardial infarction, cardiac events, etc.
```

---

## LangChain Framework

### 1. Document Loaders

**Purpose**: Load documents from various sources.

**Types used**:
- `PyPDFLoader`: Load single PDF file
- `DirectoryLoader`: Load all files from a directory

**Syntax**:
```python
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader

# Load all PDFs from a directory
loader = DirectoryLoader(
    'data/',
    glob='*.pdf',
    loader_cls=PyPDFLoader
)
documents = loader.load()

# Returns: List[Document]
# Each Document has:
#   - page_content: str (the text)
#   - metadata: dict (source, page number, etc.)
```

---

### 2. Text Splitters

**Purpose**: Split large documents into smaller chunks.

**Why needed**:
- LLMs have token limits
- Smaller chunks = more precise retrieval
- Better for embedding quality

**RecursiveCharacterTextSplitter**:
```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,        # Max characters per chunk
    chunk_overlap=20,      # Overlap between chunks
)

chunks = text_splitter.split_documents(documents)
```

**How it works**:
1. Tries to split on paragraphs first
2. Then sentences
3. Then words
4. Maintains context with overlap

**Example**:
```
Original: "Diabetes is a chronic disease. It affects blood sugar. Treatment includes..."

Chunk 1: "Diabetes is a chronic disease. It affects blood sugar."
Chunk 2: "It affects blood sugar. Treatment includes..."
         â†‘ overlap ensures context continuity
```

---

### 3. Embeddings

**LangChain Integration**:
```python
from langchain_huggingface import HuggingFaceEmbeddings

embeddings_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

# Embed a single query
query_vector = embeddings_model.embed_query("What is diabetes?")

# Embed multiple documents
doc_vectors = embeddings_model.embed_documents([
    "Diabetes is a disease...",
    "Hypertension is high blood pressure..."
])
```

**Model details**:
- **Model**: all-MiniLM-L6-v2
- **Dimensions**: 384
- **Speed**: Very fast
- **Quality**: Good for general use
- **Size**: ~80MB

---

### 4. Vector Stores

**Purpose**: Store and search embeddings efficiently.

**Pinecone Integration**:
```python
from langchain_pinecone import PineconeVectorStore

# Create new index from documents
docsearch = PineconeVectorStore.from_documents(
    documents=text_chunks,
    embedding=embeddings_model,
    index_name="medical-chatbot"
)

# Connect to existing index
docsearch = PineconeVectorStore.from_existing_index(
    index_name="medical-chatbot",
    embedding=embeddings_model
)

# Create retriever
retriever = docsearch.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 5}  # Return top 5 results
)
```

---

### 5. Retrievers

**Purpose**: Interface for retrieving documents.

**Types**:
- `similarity`: Cosine similarity search
- `mmr`: Maximum Marginal Relevance (diversity)
- `similarity_score_threshold`: Filter by score

**Usage**:
```python
# Basic retrieval
docs = retriever.invoke("What is diabetes?")

# Returns: List[Document]
for doc in docs:
    print(doc.page_content)
    print(doc.metadata)
```

---

### 6. Prompts & Prompt Templates

**Purpose**: Structure prompts for LLMs.

**SystemMessagePromptTemplate**:
```python
from langchain_core.prompts import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate
)

system_prompt = SystemMessagePromptTemplate.from_template(
    "You are a helpful medical assistant. "
    "Context: {context}\n\n"
    "History: {history}"
)

user_prompt = HumanMessagePromptTemplate.from_template(
    "Question: {query}"
)

# Combine into chat prompt
prompt = ChatPromptTemplate.from_messages([
    system_prompt,
    user_prompt
])

# Format with variables
messages = prompt.format_messages(
    context="Diabetes is...",
    history="User asked about symptoms",
    query="What causes it?"
)
```

**Benefits**:
- Reusable templates
- Type safety
- Easy variable substitution
- Supports chat history

---

### 7. LLM Integration

**ChatGroq**:
```python
from langchain_groq import ChatGroq

llm = ChatGroq(
    model_name="llama-3.3-70b-versatile",
    temperature=0.5,  # 0=deterministic, 1=creative
    api_key=groq_api_key
)

# Invoke with messages
response = llm.invoke(messages)
print(response.content)
```

**Temperature settings**:
- `0.0-0.3`: Factual, consistent (good for medical info)
- `0.4-0.7`: Balanced (good for query variations)
- `0.8-1.0`: Creative, diverse (good for brainstorming)

---

### 8. Document Objects

**Structure**:
```python
from langchain_core.documents import Document

doc = Document(
    page_content="Diabetes is a chronic disease...",
    metadata={
        "source": "medical_book.pdf",
        "page": 42
    }
)

# Access
print(doc.page_content)  # The text
print(doc.metadata)      # Dictionary of metadata
```

---

## Vector Databases & Embeddings

### 1. Pinecone

**What it is**: Cloud-based vector database for similarity search.

**Key concepts**:

**Index**: Container for vectors
```python
from pinecone import Pinecone, ServerlessSpec

pc = Pinecone(api_key=pinecone_api_key)

# Create index
pc.create_index(
    name="medical-chatbot",
    dimension=384,           # Must match embedding dimension
    metric="cosine",         # Similarity metric
    spec=ServerlessSpec(
        cloud="aws",
        region="us-east-1"
    )
)
```

**Metrics**:
- `cosine`: Measures angle between vectors (most common)
- `euclidean`: Measures distance
- `dotproduct`: Inner product

**Serverless vs Pod**:
- **Serverless**: Pay per use, auto-scaling
- **Pod**: Fixed capacity, predictable cost

---

### 2. Similarity Metrics

**Cosine Similarity**:
```
similarity = (A Â· B) / (||A|| Ã— ||B||)

Range: -1 to 1
1 = identical
0 = orthogonal
-1 = opposite
```

**Example**:
```python
import numpy as np

vec1 = np.array([1, 2, 3])
vec2 = np.array([2, 4, 6])  # Same direction

cosine_sim = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
# Result: 1.0 (very similar)
```

---

### 3. Embedding Models

**sentence-transformers/all-MiniLM-L6-v2**:

**Specifications**:
- **Type**: Sentence transformer
- **Dimensions**: 384
- **Max tokens**: 256
- **Training**: Trained on 1B+ sentence pairs
- **Use case**: General-purpose semantic similarity

**Alternatives**:
- `all-mpnet-base-v2`: 768 dim, higher quality, slower
- `all-MiniLM-L12-v2`: 384 dim, better than L6
- `text-embedding-ada-002`: OpenAI, 1536 dim, paid

---

## Advanced RAG Techniques

### 1. Query Expansion

**Concept**: Generate multiple versions of user query.

**Benefits**:
- Captures different terminology
- Handles ambiguous queries
- Improves recall

**Implementation**:
```python
def generate_query_variations(original_query: str, num_variations: int = 5):
    llm = ChatGroq(
        model_name="llama-3.3-70b-versatile",
        temperature=0.7  # Higher for diversity
    )
    
    prompt = ChatPromptTemplate.from_messages([
        SystemMessagePromptTemplate.from_template(
            "Generate {num_variations} alternative medical questions for: {query}"
        )
    ])
    
    response = llm.invoke(prompt.format_messages(
        query=original_query,
        num_variations=num_variations
    ))
    
    variations = response.content.strip().split('\n')
    return [original_query] + variations[:num_variations]
```

---

### 2. Re-ranking

**Purpose**: Improve relevance of retrieved documents.

**Methods**:
1. **RRF** (used in your project): Rank-based fusion
2. **Cross-encoder**: Neural re-ranker (more accurate, slower)
3. **MMR**: Maximum Marginal Relevance (diversity)

**RRF Implementation**:
```python
from collections import defaultdict

def reciprocal_rank_fusion(doc_lists: List[List[any]], k: int = 60):
    doc_scores = defaultdict(float)
    doc_objects = {}
    
    for doc_list in doc_lists:
        for rank, doc in enumerate(doc_list, start=1):
            doc_id = doc.page_content
            doc_scores[doc_id] += 1.0 / (k + rank)
            doc_objects[doc_id] = doc
    
    ranked_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)
    return [(doc_objects[doc_id], score) for doc_id, score in ranked_docs]
```

---

### 3. Context Window Management

**Challenge**: LLMs have token limits.

**Solution**: Select most relevant context.

**Strategies**:
1. **Top-K selection**: Take top 6 chunks (your approach)
2. **Token-based**: Fill context until token limit
3. **Sliding window**: For long documents
4. **Hierarchical**: Summarize then detail

**In your project**:
```python
# Select top 6 chunks after re-ranking
top_docs = ranked_docs[:6]

# Build context
context = "\n\n".join([doc.page_content for doc, score in top_docs])

# Add to prompt
prompt = f"Context: {context}\n\nQuestion: {query}"
```

---

### 4. Conversation Memory

**Purpose**: Maintain context across multiple turns.

**Implementation**:
```python
# Store in Flask session
session['chat_history'] = [
    {'role': 'user', 'content': 'What is diabetes?'},
    {'role': 'assistant', 'content': 'Diabetes is...'},
]

# Build history string
history_text = ""
for msg in session['chat_history'][-6:]:  # Last 3 exchanges
    role = "User" if msg['role'] == 'user' else "Assistant"
    history_text += f"{role}: {msg['content']}\n"

# Include in prompt
system_prompt = f"History:\n{history_text}\n\nContext: {context}"
```

**Best practices**:
- Limit history length (avoid token overflow)
- Store in session (user-specific)
- Clear when context changes

---

## LLM Integration

### 1. Groq

**What it is**: Fast LLM inference platform.

**Models available**:
- `llama-3.3-70b-versatile`: Best quality
- `llama-3.1-8b-instant`: Fastest
- `mixtral-8x7b-32768`: Long context

**Integration**:
```python
from langchain_groq import ChatGroq

llm = ChatGroq(
    model_name="llama-3.3-70b-versatile",
    temperature=0.5,
    api_key=os.getenv("GROQ_API_KEY")
)

# Simple invoke
response = llm.invoke("What is diabetes?")

# With messages
from langchain_core.messages import HumanMessage, SystemMessage

messages = [
    SystemMessage(content="You are a medical assistant"),
    HumanMessage(content="What is diabetes?")
]
response = llm.invoke(messages)
```

---

### 2. Prompt Engineering

**System prompts**:
```python
system_prompt = """You are a helpful medical assistant.
Based on the context below, answer concisely and clearly.
If you don't have enough information, say so.
Use three sentences maximum.

Context: {context}
History: {history}
"""
```

**Best practices**:
- Clear role definition
- Explicit instructions
- Output format specification
- Fallback behavior

---

### 3. Temperature & Parameters

**Temperature**:
- Controls randomness
- `0.0`: Deterministic (same input â†’ same output)
- `1.0`: Maximum creativity

**When to use**:
- `0.0-0.3`: Factual answers (medical info)
- `0.5-0.7`: Query variations, summaries
- `0.8-1.0`: Creative writing, brainstorming

**Other parameters**:
```python
llm = ChatGroq(
    model_name="llama-3.3-70b-versatile",
    temperature=0.5,
    max_tokens=500,      # Max response length
    top_p=0.9,           # Nucleus sampling
    frequency_penalty=0, # Reduce repetition
)
```

---

## Web Development

### 1. Flask Basics

**Setup**:
```python
from flask import Flask, render_template, request, jsonify, session

app = Flask(__name__)
app.secret_key = 'your_secret_key'

# Routes
@app.route('/')
def index():
    return render_template('chat.html')

@app.route('/get', methods=['POST'])
def chat():
    query = request.form['msg']
    response = generate_response(query)
    return str(response)

if __name__ == '__main__':
    app.run(debug=True)
```

---

### 2. Session Management

**Purpose**: Store user-specific data (chat history).

**Usage**:
```python
from flask import session

# Initialize
if 'chat_history' not in session:
    session['chat_history'] = []

# Add to session
session['chat_history'].append({
    'role': 'user',
    'content': query
})

# Mark as modified (important!)
session.modified = True

# Limit size
if len(session['chat_history']) > 20:
    session['chat_history'] = session['chat_history'][-20:]
```

---

### 3. AJAX Requests

**Frontend (jQuery)**:
```javascript
$.ajax({
    data: { msg: rawText },
    type: "POST",
    url: "/get",
}).done(function(data) {
    // Handle response
    displayBotMessage(data);
}).fail(function(xhr, status, error) {
    // Handle error
    console.error(error);
});
```

**Backend**:
```python
@app.route('/get', methods=['POST'])
def chat():
    query = request.form['msg']
    response = generate_response(query)
    return str(response)  # Returns plain text
```

---

### 4. Frontend Design

**Glassmorphism CSS**:
```css
.card {
    background: rgba(255, 255, 255, 0.1);
    backdrop-filter: blur(20px);
    border: 1px solid rgba(255, 255, 255, 0.2);
    box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
}
```

**Gradient backgrounds**:
```css
body {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    background-size: 400% 400%;
    animation: gradientShift 15s ease infinite;
}

@keyframes gradientShift {
    0% { background-position: 0% 50%; }
    50% { background-position: 100% 50%; }
    100% { background-position: 0% 50%; }
}
```

---

## System Architecture

### 1. Threading

**Purpose**: Background initialization for instant startup.

**Implementation**:
```python
import threading

# Global variables
embeddings_model = None
retriever = None
initialization_complete = False
initialization_lock = threading.Lock()

def initialize_models():
    global embeddings_model, retriever, initialization_complete
    
    embeddings_model = download_embeddings_model()
    docsearch = PineconeVectorStore.from_existing_index(...)
    retriever = docsearch.as_retriever(...)
    
    with initialization_lock:
        initialization_complete = True

# Start in background
if __name__ == '__main__':
    init_thread = threading.Thread(target=initialize_models, daemon=True)
    init_thread.start()
    app.run(debug=True, use_reloader=False)
```

**Key concepts**:
- `daemon=True`: Thread dies with main program
- `use_reloader=False`: Prevent double initialization
- `threading.Lock()`: Thread-safe flag updates

---

### 2. Environment Variables

**Purpose**: Secure API key management.

**.env file**:
```
PINECONE_API_KEY=your_key_here
GROQ_API_KEY=your_key_here
PINECONE_INDEX_NAME=medical-chatbot
```

**Loading**:
```python
from dotenv import load_dotenv
import os

load_dotenv()

pinecone_api_key = os.getenv('PINECONE_API_KEY')
groq_api_key = os.getenv('GROQ_API_KEY')
```

**Best practices**:
- Never commit `.env` to git
- Use `.env.example` as template
- Validate keys on startup

---

### 3. Error Handling

**Graceful degradation**:
```python
@app.route('/get', methods=['POST'])
def chat():
    if not initialization_complete:
        if not wait_for_initialization(timeout=30):
            return jsonify({
                'error': 'System still initializing. Please wait.'
            }), 503
    
    try:
        response = generation(query, retriever, ...)
        return str(response)
    except Exception as e:
        print(f"Error: {e}")
        return str("Sorry, I encountered an error.")
```

---

## Algorithms & Data Structures

### 1. defaultdict

**Purpose**: Dictionary with default values.

**Usage**:
```python
from collections import defaultdict

# Regular dict - KeyError if key doesn't exist
scores = {}
scores['doc1'] += 1  # âŒ KeyError

# defaultdict - auto-initializes
scores = defaultdict(float)
scores['doc1'] += 1  # âœ… Works! Starts at 0.0
```

**In RRF**:
```python
doc_scores = defaultdict(float)

for doc_list in doc_lists:
    for rank, doc in enumerate(doc_list, start=1):
        doc_id = doc.page_content
        doc_scores[doc_id] += 1.0 / (k + rank)  # Auto-initializes to 0.0
```

---

### 2. List Comprehensions

**Syntax**:
```python
# Basic
squares = [x**2 for x in range(10)]

# With condition
evens = [x for x in range(10) if x % 2 == 0]

# Nested
matrix = [[i*j for j in range(3)] for i in range(3)]
```

**In your project**:
```python
# Extract page content
context = "\n\n".join([doc.page_content for doc, score in top_docs])

# Filter variations
variations = [line.strip() for line in response.content.split('\n') if line.strip()]
```

---

### 3. enumerate()

**Purpose**: Loop with index.

**Usage**:
```python
items = ['a', 'b', 'c']

# Without enumerate
for i in range(len(items)):
    print(i, items[i])

# With enumerate (better!)
for i, item in enumerate(items):
    print(i, item)

# Start from 1
for rank, item in enumerate(items, start=1):
    print(rank, item)  # 1, 2, 3...
```

---

### 4. Lambda Functions

**Syntax**:
```python
# Regular function
def square(x):
    return x ** 2

# Lambda (anonymous function)
square = lambda x: x ** 2
```

**In sorting**:
```python
# Sort by second element of tuple
ranked_docs = sorted(
    doc_scores.items(),
    key=lambda x: x[1],  # x[1] is the score
    reverse=True
)
```

---

### 5. Type Hints

**Purpose**: Document expected types.

**Syntax**:
```python
from typing import List, Dict, Tuple

def process_docs(
    docs: List[Document],
    k: int = 60
) -> List[Tuple[Document, float]]:
    # Function body
    return results
```

**Benefits**:
- Better IDE autocomplete
- Catch type errors early
- Self-documenting code

---

## Key Takeaways

### Technical Skills Learned

1. **RAG Architecture**
   - Document loading and chunking
   - Embedding generation
   - Vector storage and retrieval
   - Context augmentation

2. **Advanced RAG**
   - Multi-query retrieval
   - Reciprocal Rank Fusion
   - Re-ranking algorithms
   - Conversation memory

3. **LangChain**
   - Document loaders
   - Text splitters
   - Embeddings
   - Vector stores
   - Retrievers
   - Prompt templates
   - LLM integration

4. **Vector Databases**
   - Pinecone setup
   - Index management
   - Similarity search
   - Serverless architecture

5. **Web Development**
   - Flask routing
   - Session management
   - AJAX requests
   - Modern UI design

6. **System Design**
   - Background threading
   - Error handling
   - Environment variables
   - Performance optimization

### Best Practices

âœ… **Security**
- Use environment variables for API keys
- Never commit secrets to git
- Validate user input

âœ… **Performance**
- Background initialization
- Efficient re-ranking
- Limited context windows

âœ… **User Experience**
- Instant UI loading
- Visual feedback (typing indicator)
- Clear error messages
- Responsive design

âœ… **Code Quality**
- Type hints
- Docstrings
- Error handling
- Modular design

---

## Resources for Further Learning

### Documentation
- [LangChain Docs](https://python.langchain.com/)
- [Pinecone Docs](https://docs.pinecone.io/)
- [Groq Docs](https://console.groq.com/docs)
- [Flask Docs](https://flask.palletsprojects.com/)

### Papers
- "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks" (RAG paper)
- "Reciprocal Rank Fusion outperforms Condorcet" (RRF paper)
- "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"

### Tutorials
- LangChain RAG tutorials
- Vector database comparisons
- Prompt engineering guides
- Flask web development

---

## Project Statistics

**Lines of Code**: ~500+  
**Technologies**: 8+ (Flask, LangChain, Pinecone, Groq, etc.)  
**Concepts Learned**: 30+  
**API Integrations**: 2 (Groq, Pinecone)  
**Advanced Techniques**: Multi-Query RAG, RRF, Threading  

---

**Congratulations on building an advanced RAG system! ðŸŽ‰**

This project demonstrates production-level skills in:
- AI/ML engineering
- Backend development
- System architecture
- Algorithm implementation
- Modern web design

Keep building and learning! ðŸš€
